{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPyMIPoFZ3/5VBmJVswfww0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wilsonfalcao/desafio-dio-conversando-por-voz/blob/main/Entrega_do_Desafio_Conversando_por_Voz_Com_o_ChatGPT_Utilizando_Whisper_(OpenAI)_e_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Desafio: Conversando por Voz com o ChatGPT via Whisper (OpenAI) e Python\n",
        "\n",
        "Bem-vindo(a)! Este reposit√≥rio cont√©m a entrega do desafio de implementa√ß√£o de uma interface de voz utilizando o modelo **Whisper** da OpenAI para transcri√ß√£o de √°udio, integrado ao ambiente do Google Colab.\n",
        "\n",
        "## üéØ Objetivo\n",
        "O foco principal deste projeto √© a **transcri√ß√£o de √°udio em texto**, explorando a capacidade de processamento de linguagem natural do modelo Whisper. A implementa√ß√£o utiliza como base os conceitos e a estrutura de c√≥digo fornecida por **Venilton**, adaptando-os para capturar √°udio diretamente do navegador e process√°-lo via Python.\n",
        "\n",
        "---\n",
        "\n",
        "## üõ†Ô∏è Tecnologias e Bibliotecas Utilizadas\n",
        "\n",
        "Para que a comunica√ß√£o entre o hardware (microfone) e o modelo de IA funcionasse perfeitamente no ambiente Cloud, foram implementadas as seguintes bibliotecas:\n",
        "\n",
        "* **`whisper`**: O motor principal da OpenAI para o reconhecimento e transcri√ß√£o da fala.\n",
        "* **`IPython.display (Javascript, Audio)`**: Utilizados para injetar scripts que permitem ao navegador gravar √°udio.\n",
        "* **`google.colab.output`**: Essencial para a comunica√ß√£o entre o back-end do Python e o front-end do notebook.\n",
        "* **`base64`**: Para decodificar os dados de √°udio capturados via JavaScript.\n",
        "* **`IPython.core.interactiveshell`**: Para ajustes na execu√ß√£o das c√©lulas do ambiente.\n",
        "\n",
        "### Trecho de Importa√ß√£o Base:\n",
        "```python\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "from IPython.display import Javascript, Audio\n",
        "from google.colab import output\n",
        "from base64 import b64decode\n",
        "import whisper"
      ],
      "metadata": {
        "id": "aZe6ZR8tRmO6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16J4yYgNBMzz",
        "outputId": "a41b18b4-5905-471a-b35f-a98c74d18926"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/openai/whisper.git -q\n",
        "\n",
        "language = 'pt'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.core.interactiveshell import dis\n",
        "from IPython.display import Javascript, Audio\n",
        "from google.colab import output\n",
        "from base64 import b64decode\n",
        "import whisper\n",
        "\n",
        "js_code = f\"\"\"\n",
        "const sleep  = time => new Promise(resolve => setTimeout(resolve, time))\n",
        "const b2text = blob => new Promise(resolve => {{\n",
        "  const reader = new FileReader()\n",
        "  reader.onloadend = e => resolve(e.srcElement.result)\n",
        "  reader.readAsDataURL(blob)\n",
        "}})\n",
        "var record = time => new Promise(async resolve => {{\n",
        "  stream = await navigator.mediaDevices.getUserMedia({{ audio: true }})\n",
        "  recorder = new MediaRecorder(stream)\n",
        "  chunks = []\n",
        "  recorder.ondataavailable = e => chunks.push(e.data)\n",
        "  recorder.start()\n",
        "  await sleep(time)\n",
        "  recorder.onstop = async ()=>{{\n",
        "    blob = new Blob(chunks)\n",
        "    text = await b2text(blob)\n",
        "    resolve(text)\n",
        "  }}\n",
        "  recorder.stop()\n",
        "  alert('grava√ß√£o encerrada');\n",
        "}})\n",
        "\n",
        "\"\"\"\n",
        "display(Javascript(js_code))\n",
        "tempo = 5000\n",
        "audio_data = output.eval_js(f\"record({tempo})\")\n",
        "audio = b64decode(audio_data.split(',')[1])\n",
        "file_name = 'request_audio.wav'\n",
        "with open(file_name, 'wb') as f:\n",
        "  f.write(audio)\n",
        "\n",
        "## Aplicando o Whisper para Transcri√ß√£o\n",
        "model = whisper.load_model(\"small\")\n",
        "\n",
        "## Transcrevendo\n",
        "result = model.transcribe(file_name, fp16=False, language=language)\n",
        "transcription = result[\"text\"]\n",
        "print(transcription)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "TdjfTHUWEkE6",
        "outputId": "01bc4659-26a1-4b51-9ed3-e8ae8d9aea34"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "const sleep  = time => new Promise(resolve => setTimeout(resolve, time))\n",
              "const b2text = blob => new Promise(resolve => {\n",
              "  const reader = new FileReader()\n",
              "  reader.onloadend = e => resolve(e.srcElement.result)\n",
              "  reader.readAsDataURL(blob)\n",
              "})\n",
              "var record = time => new Promise(async resolve => {\n",
              "  stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
              "  recorder = new MediaRecorder(stream)\n",
              "  chunks = []\n",
              "  recorder.ondataavailable = e => chunks.push(e.data)\n",
              "  recorder.start()\n",
              "  await sleep(time)\n",
              "  recorder.onstop = async ()=>{\n",
              "    blob = new Blob(chunks)\n",
              "    text = await b2text(blob)\n",
              "    resolve(text)\n",
              "  }\n",
              "  recorder.stop()\n",
              "  alert('grava√ß√£o encerrada');\n",
              "})\n",
              "\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Som, som, teste, esse √© um teste para entrega do desafio.\n"
          ]
        }
      ]
    }
  ]
}